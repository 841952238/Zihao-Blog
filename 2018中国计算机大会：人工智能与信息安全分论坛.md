> 本论坛是2018中国计算机大会的分论坛之一，涉及人工智能应用中的神经网络攻击、对抗样本生成与模型迁移、模型逆向破解、白盒与黑盒攻防策略，以及移动设备AI应用未来的信息安全挑战。
>
> 作者：张子豪（同济大学在读研究生）



金秋十月，丹桂飘香，由CCF主办，杭州市萧山区政府、浙江大学协办的2018中国计算机大会（CNCC）在杭州国际博览中心隆重召开。10月25日下午13：30，召开了“人工智能与信息安全”分论坛。AI安全攻防领域的顶尖学者齐聚一堂，共话信息安全的“芯”挑战。

分论坛主席为浙江大学电气工程学院徐文渊教授，共同主席为复旦大学软件学院副院长韩伟力教授，分享嘉宾可谓大牛云集：360智能安全研究院负责人李康、NIPS对抗样本攻防竞赛清华三连冠团队指导老师朱军、中国科学院大学教授陈恺、阿里巴巴安全研究员陆全博士。

![2018中国计算机大会：人工智能与信息安全分会场](https://upload-images.jianshu.io/upload_images/13714448-6fb364b530b76cb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

[TOC]



## 报告一  李康：人工智能系统实现中的安全风险

第一位报告人是360智能安全研究院负责人李康。分享题目是“人工智能系统实现中的安全风险”。本次报告通过丰富的攻防案例展示了人工智能应用中的安全风险及应对方法。

> 嘉宾简介：李康，360智能安全研究院负责人。清华计算机本科，耶鲁大学法学硕士，俄勒冈研究院计算机博士。主要研究系统与网络安全。李博士是网络安全对抗赛CTF最早的实践者，他是XCTF联赛的联合发起人，并担任清华大学蓝莲花战队的启蒙老师。



### 逃逸攻击与对抗样本

早在2015年，“生成对抗神经网络GAN之父”Ian Goodfellow在ICLR会议上展示了攻击神经网络欺骗成功的案例，在原版大熊猫图片中加入肉眼难以发现的干扰，生成对抗样本。就可以让Google训练的神经网络误认为它99.3%是长臂猿。

![大熊猫变长臂猿](https://upload-images.jianshu.io/upload_images/13714448-a376255416a12da2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



除此之外，人工智能系统还会面对模型推断攻击、拒绝服务攻击、传感器攻击等多种信息安全挑战。
![逃逸攻击](http://blogs.360.cn/wp-content/uploads/2017/10/1-2-1024x450.png)

### 对抗样本对预处理方式的敏感性

​    其实，如果你把那张经过攻击篡改之后的大熊猫图片稍微放大或缩小，或者直接截一部分图，然后放到其它公开的图像识别模型上运行（比如百度识图），识别结果依旧是大熊猫。这意味着对抗样本仅对指定的图片和攻击模型生效，对诸如区域截图、放大缩小之类的预处理过程是非常敏感的。也就是说，如果还想欺骗更多其它的深度学习模型，就要通过逆推破解不同深度学习模型的图片预处理方法，找到发生图像识别错误的那种预处理方法，然后应用在原图片上。
![用百度识图识别经过对抗样本攻击之后的大熊猫图片，识别结果依旧是大熊猫](https://upload-images.jianshu.io/upload_images/13714448-14e6c241fb00614e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​    人工智能系统中的潜在威胁不仅仅来自对抗样本攻击。现实中的人工智能系统会面临软件实现、数据流处理、模型供应链、本地部署应用、用户端破解等方面的各种挑战。

​    人工智能与信息安全的下一个热点：深度学习模型参数被窃取的风险和数据安全。
​    随着边缘计算和智能移动终端时代的到来，在移动终端部署本地AI应用越来越广泛，从iPhone X的刷脸解锁，到华为、高通部署手机端的AI芯片。在移动终端本地运行AI应用，可有效解决延迟、传输带宽、用户隐私泄露等问题，但同时也带来本地深度学习模型的数据安全问题。经过简单的逆推，就可以破解很多本地的AI应用，甚至可以知道其中的Caffe模型的基本参数，有些开发者会采用AES加密把模型封装起来，但殊不知在AES密钥也得保存在本地文件中。有时甚至根据追踪AI应用对内存的访问情况，就可以判断出这个模型的神经网络结构。所以AI开发者在向移动端和嵌入式设备中部署AI应用时，一定要事先请教安全团队，确保模型数据安全。

![人工智能模型数据安全](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1540498892027&di=e256bca025813be17acc3c1819527b34&imgtype=0&src=http%3A%2F%2F03imgmini.eastday.com%2Fmobile%2F20180916%2F20180916180412_19afbee60d624332f0db6fb897b9e01f_1.jpeg)



## 报告二  朱军：深度学习中的对抗攻击与防守

第二位报告人是清华大学计算机学院朱军教授。分享题目是“深度学习中的对抗攻击与防守”。本次报告通过2017NIPS对抗样本攻防竞赛案例讲述了对抗样本的生成策略、白盒与黑盒攻击、攻防模型的迁移及优化。

> 嘉宾简介：清华大学计算机系教授，清华大学人工智能研究院院长助理。清华计算机本科、博士，卡耐基梅隆大学博士后。主要从事机器学习基础理论算法及应用研究。入选国家“万人计划”青年拔尖人才。
>
> 2017，Ian Goodfellow 等研究者牵头组织了NIPS的 Adversarial Attacks and Defences（对抗攻击防御）竞赛，清华大学博士生董胤蓬、廖方舟、庞天宇及指导老师朱军、胡晓林、李建民、苏航组成的团队在竞赛中的全部三个项目中得到冠军。

机器学习系统的逃逸攻击，它能绕过深度学习的判别并生成欺骗结果。攻击者在原图上构造的修改被称为“对抗样本”。逃逸攻击可分为白盒攻击和黑盒攻击。白盒攻击是在已经获取机器学习模型内部的所有信息和参数上进行攻击，令损失函数最大，直接计算得到对抗样本；黑盒攻击则是在神经网络结构为黑箱时，仅通过模型的输入和输出，逆推生成对抗样本。

![白盒攻击与黑盒攻击](https://upload-images.jianshu.io/upload_images/13714448-04d9ff6afa457791.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![2017NIPS对抗样本攻防竞赛案例：阿尔卑斯山图片篡改后被神经网络误判为狗、河豚被误判为螃蟹。对抗样本不仅仅对图片和神经网络适用，对SVM、决策树等算法也有效](https://upload-images.jianshu.io/upload_images/13714448-073423e4463b0068.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对抗样本不仅仅对模型的最后预测产生误导，而是从特征提取过程就产生误导，下图展示了第147号神经元在正常深度学习模型和对抗样本中的关注区域。在正常模型中，第147号神经元重点关注小鸟的头部信息。在对抗样本中，第147号神经元则完全被误导了，关注的区域杂乱无章。同时也说明，**对抗样本不是根据语义生成的**，它并不智能。

 ![不仅仅对最后的预测产生误导，而是从特征提取开始就产生误导](https://upload-images.jianshu.io/upload_images/13714448-73a71d45ff8a78fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 降噪优化策略

​    对抗样本会在原图上增加肉眼很难发现的干扰，但依旧能看得出来和原图的区别：
![干扰样本对图像的改变](https://upload-images.jianshu.io/upload_images/13714448-c0a686a55b828825.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
​    一个很自然的想法就是，通过像素级别的去噪，把干扰样本图片还原回原来的图片，但经过试验，发现成功率很低。
![传统去噪成功率很低](https://upload-images.jianshu.io/upload_images/13714448-fcb3230bb04f5984.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
​    研究人员进而把去噪的目标从像素级别还原回原来的图片，改为对噪音进行滤波，引入动量Momentum，成功率大大提高，并大大提高了模型的可迁移性。见下图黑色LGD线，已与标准样本十分接近。
![改进去噪方式之后，成功率大大提高](https://upload-images.jianshu.io/upload_images/13714448-5015a0605adece99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
​    

## 报告三  陈恺：人工智能时代下的安全攻防

第三位报告人是中国科学院信息工程研究院陈恺。分享题目是“人工智能时代下的安全攻防”。本次报告讲述了人工智能在网络安全工作中的应用，并通过对歌曲进行干扰生成错误语音指令进而发动攻击的例子，讲述了人工智能算法的脆弱性与未来攻防发展趋势。

> 嘉宾简介：中国科学院信息工程研究所研究员，中国科学院大学教授、博士生导师。信息安全国家重点实验室副主任，《Cybersecurity》编辑部主任。国家“万人计划”青年拔尖人才、北京市“科技新星”。2010年获中国科学院研究生博士学位，美国宾州州立大学博士后。中国保密协会隐私保护专业委员会委员，中国计算机学会系统软件专委会委员。主要研究领域包括软件与系统安全、人工智能安全。在IEEE S&P、USENIX Security、ACM CSS、ICSE、ASE等发表论文70余篇；曾主持和参加国家重点研发计划、国家自然科学基金、863计划等国家部委课题40余项。

在报告中，陈恺教授展示了他们的最新成果：对汽车音响播放的歌曲进行干扰编码，在人耳听起来仍然是原曲的情况下就可以让微信的语音输入法获得错误的“Open the door”指令。但歌曲很容易受外界噪音干扰。本文作者张子豪提出可以使用树莓派微型电脑发射FM调频广播播放干扰之后的歌曲，直接干扰汽车收音机，陈恺博士高度赞赏了这个建议并表示他们已经尝试过这个方法，但决定干扰成功率的关键还是在于过滤外界噪音干扰。

## 报告四  陆全：基于AI的文本分析技术在安全的应用及研究

第四位报告人是阿里巴巴安全部门的陆全。分享题目是“基于AI的文本分析技术在安全的应用及研究”。本次报告通过文本分析和处理技术在阿里安全的几个场景的应用，包括敏感文本信息识别，情报舆情感知，欺诈短信检测、WAF攻击检测等。并着重以欺诈短信检测为例，结合实际中的三个主要挑战：借助人工标注提升覆盖率，构造训练样本提升准确率，应对变异和对抗提高鲁棒性，来介绍我们的技术和成果。

> 嘉宾简介：美国南加州大学博士，负责集团系统和数据安全领域的开发。有十余年将大数据统计机器学习，数据挖掘和深度学习成功引用的业界经验，曾在雅虎和Explain带领研发团队。并在顶级国际会议上发表论文二十余篇，拥有多项国际专利。

在报告中，陆全教授展示了如何识别欺诈短信中的恶意微信号推广，比如“加薇”、“+V芯”、“珈 威❤”。阿里安全部门通过人工标注大大提高了检测覆盖率，并针对训练样本优化了模型的可迁移性和鲁棒性。



## 现场观众提问

### 如何针对小信息量数据的生成对抗样本

- 问：讲座中提到的神经网络攻击，都是针对图片、语音这些包含丰富冗余信息量数据的。那么对于小信息量的数据，比如二维码图片，能否生成对抗样本呢？
- 答：这个问题提的很好。图片、语音数据恰恰是因为包含丰富的冗余信息，在加入对抗样本之后才不会被人察觉，而二维码这种数据稍加篡改就会被肉眼察觉。而且在安全领域，已经有许多成熟的针对二维码的攻击方案，最原始的就是有人把摩拜单车上的二维码撕下来换成自己的收款码，或者把别人的付款码打印下来用于消费。

### 能否通过污染训练数据集进行攻击

- 问：能否直接引入大量污染的数据，针对深度学习训练用的数据集进行攻击？
- 答：可以。这是釜底抽薪的攻击，也叫“后门攻击”。Machine Learning和Machine Teaching从来不是割裂的。在防御角度，需要“纵深防御”，同时这也是“连续学习”的一部分。



## 参考文献与扩展阅读

> [如何优雅地欺骗神经网络，让人工智能指鹿为马（同济大学张子豪）](https://github.com/TommyZihao/Zihao-Blog/blob/master/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E5%9C%B0%E6%AC%BA%E9%AA%97%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%8C%E8%AE%A9%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8C%87%E9%B9%BF%E4%B8%BA%E9%A9%AC%20%E2%80%942018%E4%B8%AD%E5%9B%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E4%BC%9A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E5%88%86%E8%AE%BA%E5%9D%9B.md)
>
> [用人话讲解FGSM算法（同济大学张子豪）](https://github.com/TommyZihao/Zihao-Blog)
>
> [清华大学团队包揽三项冠军，NIPS 2017对抗样本攻防竞赛总结](https://www.leiphone.com/news/201804/WcmoNd6pO4bTQ1yV.html)
>
> [Adversarial Attacks and Defences Competition](https://arxiv.org/abs/1804.00097?utm_campaign=ARCHITECHT&utm_medium=web&utm_source=ARCHITECHT_15)
>
> [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
>
> [Adversarial Examples that Fool both Computer Vision and Time-Limited Humans](https://arxiv.org/abs/1802.08195)
>
> [Goodfellow最新对抗样本，连人类都分不清是狗是猫](https://www.leiphone.com/news/201802/85vvpmuoNRT8gZHD.html)
>
> [动量迭代攻击和高层引导去噪：对抗样本攻防的新方法](http://www.mooc.ai/open/course/383)
>
> [清华大学廖方舟：产生和防御对抗样本的新方法 | 分享总结](https://www.leiphone.com/news/201801/eqwoT6Q4KFzXtjyy.html)
>
> [两分钟论文：对抗样本同时骗过人类和计算机视觉 @雷锋字幕组](https://v.qq.com/x/page/n0638ta39r3.html?start=88)
>
> [谷歌新论文发现：对抗样本也会骗人](https://www.leiphone.com/news/201804/6ZCljeQmSYKFGetg.html)
