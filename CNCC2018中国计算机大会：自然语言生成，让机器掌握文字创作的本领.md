> 本论坛是2018中国计算机大会（CNCC）的分论坛之一：自然语言生成，让机器掌握文字创作的本领。涉及自然语言生成目前成果总结、产业应用及前景展望。包括微软小冰、阿里小蜜、高考议论文自动生成、腾讯新闻推荐系统、财经及体育新闻生成等应用案例及背后的优化算法模型。
>
> 作者：张子豪（同济大学在读研究生）
>
> 微信公众号：人工智能小技巧
>
> 发布于2018-10-31

随着深度计算的发展，自然语言生成获得了新的发展机遇。从2015年开始，腾讯、今日头条、南方都市报等先后采用写稿机器人，单篇成文的速度可达到0.5秒。2014年以来，自然对话也被认为是下一代人机交互的关键技术获得了长足发展，开放域聊天机器人（如微软小冰）和客服型机器人（如阿里小蜜）均有上线的产品和海量的用户。对话生成近年来也在自然语言处理会议上逐渐升温。除此之外，研究人员对其他问题也做出尝试。2017年人类历史上第一本百分之百人工智能创作的诗集《阳光失了玻璃窗》正式出版，作者正是微软小冰。在2018年中国计算机大会（CNCC）“自然语言生成：让机器掌握文字创作的本领”分论坛，自然语言处理领域的顶尖学者齐聚一堂，共话机器文本处理的“芯”挑战。

![阳光失了玻璃窗](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1540886303653&di=d42329be860875b2287bae3880ce896f&imgtype=0&src=http%3A%2F%2Fimg.zcool.cn%2Fcommunity%2F017e2059c4b271a801218e187a34a7.jpg%401280w_1l_2o_100sh.jpg)

时间：2018年10月26日 下午13:30-17:30

地点：杭州国际博览中心会议区  二层新闻发布厅 A

分论坛主席为北京大学计算机科学技术研究所研究员万小军、微软（亚洲）互联网工程院小冰首席科学家宋睿华。分享嘉宾可谓大牛云集：哈工大计算机学院教授秦兵、腾讯新闻产品技术部算法中心总监范欣、微软亚洲研究院自然语言计算组首席科学家武威、清华大学计算机系副教授黄民烈。

[TOC]

# 报告一  万小军：NLG自然语言生成目前成果总结及前瞻展望

![万小军](https://upload-images.jianshu.io/upload_images/13714448-f9d7bd654297d0ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 嘉宾简介：万小军，北京大学计算机科学技术研究所研究员，博士生导师，语言计算与互联网挖掘研究室负责人。研究方向为自然语言处理与文本挖掘，研究兴趣包括自动文摘与文本生成、情感分析与语义计算等。担任计算语言学顶级国际期刊Computational Linguistics编委，TACL常务评审，多次担任自然语言处理与人工智能领域一流国际会议领域主席（Area Chair）或高级程序委员（SPC），包括ACL、NAACL、EMNLP、IJCAI、AAAI等。荣获ACL2017杰出论文奖、IJCAI2018杰出论文奖、2017年吴文俊人工智能技术发明奖、CCF NLPCC青年新锐奖等多项荣誉或奖励。

### 自然语言生成NLG的四种典型应用场景：

自然语言生成的产业应用主要包括四种场景：第一种是从文本到文本（模仿写作风格写新作品） ，第二种是从结构化数据到文本（财经新闻、体育比赛新闻生成），第三种是由非结构化数据的文本生成（给图片或视频打标签），第四种是原创与二次创作（根据文摘写摘要，或创作剧本小说）。虽然已经出现许多成功的应用，比如微软小冰、阿里小蜜、《南方周末》写稿机器人，但目前NLG领域总体上还处于弱人工智能水平。

### NLG未来发展趋势与面对的挑战
与计算机视觉、机器翻译领域不同，自然语言生成领域缺少高质量数据、人工标注很困难，如何由少量数据生成可靠模型是未来的首要发展方向。

 

目前已经有成熟的摘要写作、财经新闻写作、体育比赛新闻写作的写稿机器人面世，但都没有独特的写作风格，如何实现不同写作文本风格转换并能通过“图灵检测”，是NLG未来发展的一大趋势。

另一个发展方向是读取长文本生成短文本，比如将学术论文转为简洁易懂的科普文章，目前已有相关的需求，但尚无成熟的解决方案。

实时处理在NLG中也非常重要，比如世界杯足球赛实时解说、机器辩论，再到专家谈判系统，输出实时语音流也会是NLG未来的热点之一。

高考作文、小说剧本创作，是指定话题的长文本写作，未来需求也会很旺盛。但目前，模板填充是主要方法，算法填充仅起辅助作用，仍然跳不出模板的框架。

跨模态文本生成（比如由音乐生成歌词）作为NLG的一个分支，随着多媒体数字化和人工智能作曲的发展，将会有更多应用。

除此之外，自然语言生成领域缺乏统一的评价指标体系，这也是未来一大缺口。






# 报告二  秦兵：高考作文中的议论文自动题意分析及生成

![秦兵](https://upload-images.jianshu.io/upload_images/13714448-aab48cc33dad09a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

第二位报告人是哈尔滨工业大学计算机学院秦兵教授。分享题目是“高考作文中的议论文自动题意分析及生成”。高考议论文相比于其他文本生成任务，更加侧重于篇章的论点分析及篇章生成的内容组织。本次讨论重点关注高考作文中的立意分析、根据题意分析获得的论点及分论点以及进一步在此基础上生成议论文技术。

> 嘉宾简介：秦兵，哈尔滨工业大学计算机学院教授、博士生导师。哈尔滨工业大学社会计算与信息检索中心副主任。中国中文信息学会理事、中国中文信息学会语言与知识计算专委会主任，中国计算机学会中文信息技术专委会委员。主要研究方向：文本生成、知识图谱、情感分析等。在ACL、IJCAI、AAAI、IEEE TKDE、IEEE TASLP等国内外顶级会议及重要期刊上发表论文80余篇，主持多项国家基金委项目和国家科技部863项目，在NLPCC2018情感对话生成评测获得第二名，主持开发了863项目语文卷答题系统。同时和多加互联网企业开展合作，多项研究成果进入企业产品。获中文信息学会钱伟长中文信息处理科学技术奖一等奖、黑龙江省技术发明一等奖、黑龙江省技术发明二等奖
>

### 高考议论文作文的特点

高考议论文作文与一般的自然语言生成不同，因为议论文是题材性的长文本，且高质量的训练集十分有限。高考作文分为三种类型：话题作文、半命题作文、根据材料自拟题目作文，难度依次增加。

### 议论文自动生成的思路

高考议论文写作机器人的写作方法与一般考生方法是类似的。都经过了读材料、搜索素材、整合论点、梳理框架、流畅表达的过程。即理解题意、立意分析、生成论点、分论点、扩充话题词并聚类、生成作文标题、从素材库中挑选句子并排序、生成作文全文。

### 议论文生成的主要难点

议论文是带有特定题材的长文本，很难找到高质量的训练集，而且要发动很多语文老师对训练集进行打分，我们使用知乎语料作为训练集，并辅以科大讯飞作文自动打分系统，从高赞的文章中采用迁移学习的方法提取高分文本特征，再针对指定命题材料进行微调。

### 议论文生成结果评估

目前自动生成的作文，句子过于碎片化，句段之间缺乏衔接性和思维连贯性。针对连贯性和跑题问题，我们在深度学习模型中引入了注意力机制和动态覆盖机制，保证主要论点和关键词在全文都能得以体现。通过粗粒度到细粒度的文本生成框架优化议论文生成的结构和逻辑，并进一步借鉴人类的写作模式进行探索。



# 报告三  范欣：资讯内容理解和辅助创作

第三位报告人是腾讯新闻产品技术部算法中心总监范欣。分享题目是“资讯内容理解和辅助创作”。

> 嘉宾简介：范欣，腾讯新闻产品技术部算法中心总监，专家研究员。目前负责腾讯新闻的内容理解、个性化推荐和创新业务的算法。腾讯新闻写稿机器人Dreamwriter的技术负责人。2007年毕业于中科大-微软联合培养博士项目，有多年的搜索和个性化推荐产品的研究经验。

![范欣](https://upload-images.jianshu.io/upload_images/13714448-e860c6939db678c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

腾讯新闻平台拥有每天十亿级的用户阅读量，已经超过了很多官方媒体。如何为每位用户个性化推荐新闻成为了重要问题。腾讯新闻根据用户画像、兴趣标签、内容排序，构建双层的新闻知识图谱系统，对新闻内容进行结构化组织。

二次创作的流程是这样的：首先，机器对已有的新闻资讯或者原始新闻数据进行自动摘要和改写，同时自动搭配图片和视频，就形成了可读的新闻稿。腾讯新闻产品技术部算法中心利用文本、图像、视频多模态内容分析，简化了新闻内容生产流程，在报道、短视频自动生成领域进行了探索。

在未来，腾讯新闻将开发辅助编辑系统。包括基于财报、战报的快速供稿、热点素材挖掘、快速理解分析审核系统。辅助编辑系统将大大节省新闻校对审核时间，快速甄别出文章的舆情指数、敏感信息、正负能量、低俗恶俗、标题党和灌水等特征，同时通过图像和视频处理做到文章的自动配图、选图、裁剪、排版。

  

# 报告四  武威：开放域对话生成及在微软小冰中的应用

第四位报告人是微软首席科学家武威，在微软小冰项目担任算法研究工作。分享题目是“开放域对话生成及在微软小冰中的应用”。

> 嘉宾简介：武威，现任微软首席科学家，他于2012年加入微软亚洲研究院自然语言计算组，并于2016-2017年担任主管研究员。他于2007年在北京大学数学科学学院获得理学学士学位，并于2012年于北京大学数学科学学院概率统计系获得博士学位。他的研究兴趣包括自然语言处理，机器学习，以及信息检索。武威博士在ACL、EMNLP、AAAI、IJCAI、WSDM、CIKM，以及JMLR等自然语言处理、人工智能、信息检索，以及机器学习的顶级会议期刊上发表超过25篇文章，并长期担任NIPS、ICML、AAAI、IJCAI、SIGIR、WWW、WSDM、KDD、ACL等顶级会议的程序委员会委员。他目前的研究重心是自然人机对话。他为微软小冰第二代到第六代对话引擎贡献了核心算法。他最近的成就是带领团队研发了第五代小冰的生成模型以及第六代小冰的共感模型。

### 人机对话领域的挑战

人机对话是自然语言生成的热点领域，直接面对的需求就是人工智能客服与聊天机器人，目前的聊天机器人虽然能初步理解上下文，但在超长文本处理仍然很难把握。对话管理技术仍显内容性不足。在微软小冰中，通过引入解码器算法和层次循环注意力模型，显著提升了对话生成效率。

通过观察大量用户和微软小冰的交互案例，工程师总结了对话生成领域四个问题：万能回复问题、上下文建模问题、回复解码效率问题、对话管理问题。

### 万能回复问题

万能回复问题指的是在开放域对话中，使用万能答案回复各种问题，模型学习出高频万能回复词语，解决这个问题的关键在于引入外部信息和基于话题的注意力机制，生成相关概念的联想，避免毫无信息量的万能回复出现。还可以先由输入内容生成隐变量，聊天机器人的回复由输入内容和隐变量共同决定。

### 上下文建模问题

上下文建模问题指的是聊天机器人上下文内容连贯一致性问题，上下文具有字、句、段、文分层的数据结构，具有大量与回复内容无关的冗余信息，关键词联想可能与上下文根本无关，上下文存在时序和远距离依赖关系等特点。

### 层次循环注意力模型

通过循环神经网络对上下文的每一个词序列进行建模，产生每一个句子的表示，采用词级别的注意力机制，突出词对回复生成的影响。再通过循环神经网络对上下文的句子进行建模，产生上下文的表示，采用句子级别的注意力机制，突出重要句子对回复生成的影响。最后的回复生成由上下文表示、词和句的注意力机制共同实现的。这个模型可以告诉我们机器是怎样理解上下文的。通过“你为什么不能陪我去吃饭”的例子进一步说明了

###  对话生成中的解码效率问题

传统对话生成模型效率低下，一个重要原因是在回复生成中每解码一个词，模型都要扫一遍全词表，但实际上对于特定的输入，全词表中大部分内容都与回复生成无关。微软小冰团队开发了基于动词表的对话生成模型，在原有的注意力机制和解码器模型上加了过滤器，在保留功能性词汇的基础上筛选出最相关的内容性词汇。

这样每次回复生成时只需要扫描一个小型动态词表即可，经过试验可提升40%的效率。

### 开放域对话管理问题

在开放域的对话环境下，用户的意图非常分散，难以判定用户具体需求。微软小冰通过分析用户的对话技巧，开发了基于上下文的策略预测模型，分析用户对话技巧并预测用户反馈，进而通过回复预测模型，根据预测出的用户反馈产生指定回复。这个模型还可以扩展到基于话题策略的开放域对话，也可以分析用户情感状态作为回复策略的依据进行对话管理。



# 报告五  黄民烈：语言生成中的一致性和逻辑性问题

第五位报告人是清华大学计算机系副教授黄民烈，分享题目是“语言生成中的一致性和逻辑问题”。探讨了在长文本对话生成中如何保持属性一致、逻辑连贯。

> 嘉宾简介：黄民烈，清华大学计算机系副教授，博士生导师，人工智能研究所副所长。研究兴趣主要集中在自然语言处理如自动问答、对话系统、情感与情绪智能等。已超过60篇CCF A/B类论文发表在ACL、IJCAI、AAAI、EMNLP、ACM TOIS等国际顶级或主流会议及期刊上。获得IJCAI-ECAI 2018杰出论文奖，获得NLPCC 2015最佳论文，其关于情绪化聊天机器人的工作被MIT Technology Review、NVIDIA、英国卫报（The Guardian）、参考消息、新华社等媒体广泛报道。曾担任多个国际顶级会议的领域主席或高级程序委员，如AAAI2019、IJCAI2018、IJCAI2017、ACL2016、EMNLP2014/2011等。

![黄民烈与本文作者](https://upload-images.jianshu.io/upload_images/13714448-114c8389d3151cd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

微软小冰虽然是一款成功的产品，但尚有瑕疵，比如没有上下文的记忆、关键属性不明确、无自身独特人格个性，如何保证风格一致，用同样的画风呈现在用户面前。

![微软小冰的逻辑缺陷](https://upload-images.jianshu.io/upload_images/13714448-c77b5d715a44d11c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



![语义逻辑缺陷](https://upload-images.jianshu.io/upload_images/13714448-c5bb3ec60af9aeb6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

苹果的Siri也会遇到语义理解问题（目前这些bug已得到修复）。

![Siri的语言理解故障](https://upload-images.jianshu.io/upload_images/13714448-8085d1a5c0d23393.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

清华大学在交互性和逻辑一致性优化方面做了大量工作。开发了ECM聊天机器人（Emotional Chatting Machine），首次将情感因素引入了基于深度学习的生成模型中。构建图注意力的常识知识图谱编码模型，建立上下文事件联系和隐性逻辑线索关系。比如，当用户提到“万圣节”，模型就可以自动给出这个事件与“糖果”、“扮鬼”、“装扮”等关键词的联系。

 

ECM 的主要数据来源是新浪微博。但微博作为一个非常活跃的社交媒体，也有非常多涉及网络用语、反语、双关的帖子或评论，目前有不少学者在做相关的研究，包括网络新词、反语检测、双关检测等，黄民烈博士自己也有相关的研究工作。比如在自然语言处理领域顶级会议 ACL 2014 上，黄民烈博士有一篇第一作者的收录论文《情感分析中的新词发现》（New Word Finding for Sentiment Analysis），基于微博数据提出了一种数据驱动、不依赖知识、非监督的新词发现算法。2017年9月，黄民烈博士也带领清华的两位学生，联合搜狗搜索团队一举获得了全球唯一开放域对话评测比赛 NTCIR-STC2 的冠军。

在故事性长文本的续写中也能做深层理解。比如下面这个例子：输入“生火做晚饭”、“离开灶台去睡觉”，人工智能就能理解出潜在的“误事”语义，输出“当厨师回来的时候，炉子烧糊了”的续写结果。

![开放域对话生成](https://upload-images.jianshu.io/upload_images/13714448-70cb7b9771fc8148.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![根据上文预测下文](https://upload-images.jianshu.io/upload_images/13714448-9f3191fa8d10e373.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

相关论文可查看《Emotional Chatting Machine:
Emotional Conversation Generation with Internal and External Memory》，作者周昊、黄民烈、张天扬、朱小燕、刘兵。

# 作者介绍

> 张子豪，同济大学在读研究生。微信公众号“人工智能小技巧”运营者。致力于用人类能听懂的语言向大众科普人工智能前沿科技。目前正在制作《说人话的深度学习视频教程》、《零基础入门树莓派趣味编程》等视频教程。西南地区人工智能爱好者高校联盟联合创始人，重庆大学人工智能协会联合创始人。充满好奇的终身学习者、崇尚自由的开源社区贡献者、乐于向零基础分享经验的引路人、口才还不错的程序员。
>
> 说人话的零基础深度学习、数据科学视频教程、树莓派趣味开发视频教程等你来看！
>
> 微信公众号：人工智能小技巧   Github代码仓库:TommyZihao
>
> [同济大学开源软件协会](https://mirrors.tongji.edu.cn/)
>
> 西南人工智能爱好者联盟   
> 重庆大学人工智能协会     
