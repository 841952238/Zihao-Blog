> 本文用人话介绍了人工智能与信息安全的交叉前沿研究，包括用对抗样本修改图片误导神经网络指鹿为马、针对不同深度学习模型的逃逸攻击、白盒攻击与黑盒攻击、NIPS 2017神经网络对抗样本攻防竞赛，以及未来人工智能面临的信息安全挑战。部分内容来自于2018中国计算机大会人工智能与信息安全分会场报告。
>
> 作者：张子豪（同济大学在读研究生）   
>
> 微信公众号：子豪兄的科研小屋  回复“指鹿为马”即可看到本文原文
>
> 发布于2018-10-29

<font color=#0099ff size=6 face="黑体">自己人！全文无数学推导，请放心食用。</font>

[TOC]

# 1、图片干扰：人工智能秒变人工智障

![用生成对抗神经网络制作表情包：情绪逐渐变化](https://upload-images.jianshu.io/upload_images/13714448-7a888d3a407e4c49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



胖虎和吴亦凡，边界是如此的模糊。

王力宏和张学友，看上去竟如此的神似。

人脸识别、自动驾驶、刷脸支付、抓捕逃犯、美颜直播……人工智能与实体经济深度结合，彻底改变了我们的生活。神经网络和深度学习貌似强大无比，值得信赖。

殊不知，人工智能是最聪明的，却也是最笨的，其实只要略施小计就能误导最先进的深度学习模型指鹿为马。

## 大熊猫被误判为长臂猿

早在2015年，“生成对抗神经网络GAN之父”Ian Goodfellow在ICLR会议上展示了攻击神经网络欺骗成功的案例，在原版大熊猫图片中加入肉眼难以发现的干扰，生成对抗样本。就可以让Google训练的神经网络误认为它99.3%是长臂猿。

![大熊猫变长臂猿](https://upload-images.jianshu.io/upload_images/13714448-a376255416a12da2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## 阿尔卑斯山被误判为狗

2017NIPS对抗样本攻防竞赛案例：阿尔卑斯山图片篡改后被神经网络误判为狗、河豚被误判为螃蟹。对抗样本不仅仅对图片和神经网络适用，对支持向量机、决策树等算法也同样有效。

![2017NIPS对抗样本攻防竞赛案例：阿尔卑斯山图片用对抗样本干扰后被神经网络误判为狗、河豚被误判为螃蟹。对抗样本不仅仅对图片和神经网络适用，对SVM、决策树等算法也有效](https://upload-images.jianshu.io/upload_images/13714448-073423e4463b0068.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 人工智障：人工也是智障

在2018年，Ian Goodfellow再发大招，不仅欺骗了神经网络，还能欺骗人眼。详情见论文[Adversarial Examples that Fool both Computer Vision and Time-Limited Humans](https://arxiv.org/abs/1802.08195)，文中提出了首个可以欺骗人类的对抗样本。下图左图为猫咪原图，经过对抗样本干扰之后生成右图，对于右图，神经网络和人眼都认为是狗。

> 这篇论文行文流畅、通俗易懂，适合新手阅读。你也可以用两分钟观看这篇论文的[视频介绍](https://v.qq.com/txp/iframe/player.html?vid=n0638ta39r3)。  

![左图为原图猫咪，经过干扰之后生成右图，神经网络和人眼都认为是狗](https://static.leiphone.com/uploads/new/article/740_740/201802/5a927ff3eddaa.png?imageMogr2/format/jpg/quality/90)

​    下图中，绿色框为猫的原图。左上角显示了攻击的目标深度模型数量越多，生成的图像对人类来说越像狗。左下角显示了针对 10 个模型进行攻击而生成的对抗样本，当eps=8的时候，人类受试者已经把它认成狗了。

![猫还是狗？](https://static.leiphone.com/uploads/new/article/740_740/201802/5a929f8d339d4.png?imageMogr2/format/jpg/quality/90)

# 2、对抗样本：让神经网络指鹿为马

这就是对机器学习模型的逃逸攻击，它能绕过深度学习的判别并生成欺骗结果。**攻击者在原图上构造的修改被称为“对抗样本”**。神经网络对抗样本生成与攻防是一个非常有（zhuang）趣（bi）且有前景的研究方向，但常人难以轻易理解内在原理，下面子豪兄就用人话向你娓娓道来。
> 这是一篇介绍对抗样本生成基本原理的通俗易懂的文章：[对抗样本的基本原理](https://www.leiphone.com/news/201806/aLeiPZA0FbVtQI6M.html)，里面甚至教你用开源人工智能框架Keras生成对抗样本攻击知名的Inception V3模型，把猪识别成烤面包机。

![逃逸攻击](http://blogs.360.cn/wp-content/uploads/2017/10/1-2-1024x450.png)

除此之外，人工智能还面临模型推断攻击、拒绝服务攻击、传感器攻击等多种信息安全挑战。

## 白盒攻击与黑盒攻击

逃逸攻击可分为白盒攻击和黑盒攻击。白盒攻击是在已经获取机器学习模型内部的所有信息和参数上进行攻击，令损失函数最大，直接计算得到对抗样本；黑盒攻击则是在神经网络结构为黑箱时，仅通过模型的输入和输出，逆推生成对抗样本。下图左图为白盒攻击（自攻自受），右图为黑盒攻击（用他山之石攻此山之玉）。

  ![白盒攻击与黑盒攻击](https://upload-images.jianshu.io/upload_images/13714448-2403524823c839b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)





## 对抗样本改变了图像的什么？噪音！

  对抗样本会在原图上增加肉眼很难发现的干扰，但依旧能看得出来和原图的区别，下图左图为对抗样本，右图为熊猫原图。
![干扰样本对图像的改变](https://upload-images.jianshu.io/upload_images/13714448-c0a686a55b828825.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​    对抗样本不是仅在最后预测阶段产生误导，而是从特征提取过程开始就产生误导，下图展示了第147号神经元分别在正常深度学习模型和对抗样本中的关注区域。在正常模型中，第147号神经元重点关注小鸟的头部信息。在对抗样本中，第147号神经元则完全被误导了，关注的区域杂乱无章。同时也说明，**对抗样本不是根据语义生成的**，它并不智能。而且，正如接下来讲述的，对抗样本对图片预处理过程非常敏感，任何区域截图、放大缩小、更换模型都很容易让对抗样本失效。

 ![对抗样本不仅仅对最后的预测产生误导，而是从特征提取开始就产生误导](https://upload-images.jianshu.io/upload_images/13714448-73a71d45ff8a78fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## 对抗样本有多强？截个图换个模型就失效！

​    其实，如果你把那张经过攻击篡改之后的大熊猫图片稍微放大或缩小，或者直接截一部分图，然后放到其它公开的图像识别模型上运行（比如百度识图），识别结果依旧是大熊猫。这意味着对抗样本仅对指定的图片和攻击模型生效，对诸如区域截图、放大缩小之类的预处理过程是非常敏感的。也就是说，如果还想欺骗更多其它的深度学习模型，就要在训练生成对抗样本时尽可能包含更多的已知深度学习模型。


![用百度识图识别经过对抗样本攻击之后的大熊猫图片，识别结果依旧是大熊猫](https://upload-images.jianshu.io/upload_images/13714448-14e6c241fb00614e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



# 3、NIPS 2017 神经网络对抗攻防竞赛

2017年，“生成对抗神经网络GAN之父”Ian Goodfellow 牵头组织了NIPS的 Adversarial Attacks and Defences（神经网络对抗攻防竞赛），清华大学博士生董胤蓬、廖方舟、庞天宇及指导老师朱军、胡晓林、李建民、苏航组成的团队在竞赛中的全部三个项目中得到冠军。以下是清华大学参赛师生赛后撰写的总结和相关报告。

> [清华大学团队包揽三项冠军，NIPS 2017对抗样本攻防竞赛总结](https://www.leiphone.com/news/201804/WcmoNd6pO4bTQ1yV.html)    
>
> [清华大学廖方舟：产生和防御对抗样本的新方法 | 分享总结](https://www.leiphone.com/news/201801/eqwoT6Q4KFzXtjyy.html)    
>
> [朱军：深度学习中的对抗攻击与防守—2018中国计算机大会人工智能与信息安全分会场](https://github.com/TommyZihao/Zihao-Blog/blob/master/2018%E4%B8%AD%E5%9B%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E4%BC%9A%EF%BC%9A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E5%88%86%E8%AE%BA%E5%9D%9B.md)    
>
> [动量迭代攻击和高层引导去噪：对抗样本攻防的新方法](http://www.mooc.ai/open/course/383)     
>
> [清华参赛队攻击组论文：Boosting Adversarial Attacks with Momentum](https://arxiv.org/abs/1710.06081)   
>
> [清华参赛队防御组论文：Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser](https://arxiv.org/abs/1712.02976)   

神仙打架看不懂？请看本文作者张子豪撰写的另一篇人类能看得懂的科普文：[神仙打架看不懂？用人话解读NIPS神经网络攻防赛清华三连冠团队模型算法](https://github.com/TommyZihao/Zihao-Blog/blob/master/%E7%A5%9E%E4%BB%99%E6%89%93%E6%9E%B6%E7%9C%8B%E4%B8%8D%E6%87%82%EF%BC%9F%E7%94%A8%E4%BA%BA%E8%AF%9D%E8%A7%A3%E8%AF%BBNIPS%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E8%B5%9B%E6%B8%85%E5%8D%8E%E4%B8%89%E8%BF%9E%E5%86%A0%E5%9B%A2%E9%98%9F%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95.md)。这篇文章详细介绍了基本算法FGSM、攻防技巧、比赛规则、清华参赛队的模型可迁移性优化策略、降噪优化算法。

## 比赛分组规则

![NIPS2017竞赛](https://upload-images.jianshu.io/upload_images/13714448-38b1aba9b6743f95.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

比赛为三组选手互相进行攻防

- Targeted Attack组：组委会给5000张原图和每张图对应的目标误导结果数据集，指定要求指鹿为马
- Non-targeted Attack组：只要认不出是鹿就行
- Defense组：正确识别已经被其它参赛组对抗样本攻击的图片

![比赛组别](https://upload-images.jianshu.io/upload_images/13714448-c338bbbdfe3c8961.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 攻击组：对抗样本生成策略

- **采用改进的FGSM模型**：多步迭代、带目标、引入动量，大大提高对抗样本的可迁移性。
- **集合攻击—集百山之石，攻此山之玉**：攻击多个已知深度学习模型的集合，而不是逐个击破。比方说，把ResNet、VGG、Inception三个模型视作统一的大模型一起攻击，再用训练好的模型攻击AlexNet，成功率就会大大提高。可以在模型底层、预测值、损失函数三个层面进行多个模型的集合攻击。采用这个方法，可以大大提高对抗样本攻击的普适性和可迁移性。

## 防御组：图像降噪策略

- **对抗训练—把真实战场作为训练场**：在训练模型的时候就加上对抗样本（对抗训练）。对抗样本随模型训练的过程在线生成。虽然很耗时，但训练出的模型鲁棒性很强。
- **改进的HGD降噪算法**：像素层面上的去噪并不能真正去掉噪音，传统的像素去噪方法全都无效。采用基于CNN的改进HGD降噪算法，仅使用750张训练图片，大大节省训练时间，且模型可迁移性好。

# 4、邪恶的未来应用

## 攻击汽车音乐，让驾驶系统得到错误语音指令（已实现）

2018年10月25日，杭州，中国计算机大会，人工智能与信息安全分会。

第三位报告人是中国科学院大学陈恺教授。分享题目是“人工智能时代下的安全攻防”。本次报告讲述了人工智能在网络安全工作中的应用，并通过对歌曲进行干扰生成错误微信语音指令进而发动攻击的例子，讲述了人工智能算法的脆弱性与未来攻防发展趋势。

> 陈恺教授简介：中国科学院信息工程研究所研究员，中国科学院大学教授、博士生导师。信息安全国家重点实验室副主任，《Cybersecurity》编辑部主任。国家“万人计划”青年拔尖人才、北京市“科技新星”。2010年获中国科学院研究生博士学位，美国宾州州立大学博士后。中国保密协会隐私保护专业委员会委员，中国计算机学会系统软件专委会委员。主要研究领域包括软件与系统安全、人工智能安全。在IEEE S&P、USENIX Security、ACM CSS、ICSE、ASE等发表论文70余篇；曾主持和参加国家重点研发计划、国家自然科学基金、863计划等国家部委课题40余项。

在报告中，陈恺教授展示了他们的最新成果：对汽车音响播放的歌曲进行干扰编码，在人耳听起来仍然是原曲的情况下就可以让微信的语音输入法获得错误的“Open the door”指令。但歌曲很容易受外界噪音干扰。本文作者张子豪提出可以使用树莓派微型电脑发射FM调频广播播放干扰之后的歌曲，直接干扰汽车收音机，陈恺博士高度赞赏了这个建议并表示他们已经尝试过这个方法，但决定干扰成功率的关键还是在于过滤外界噪音干扰。

## 攻击人脸识别，让刷脸支付和门禁系统危机重重

## 攻击交通标志，对自动驾驶汽车识别出错误含义



# 5、参考文献与扩展阅读

> [**2018中国计算机大会：人工智能与信息安全分论坛**](https://github.com/TommyZihao/Zihao-Blog/blob/master/2018%E4%B8%AD%E5%9B%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E4%BC%9A%EF%BC%9A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E5%88%86%E8%AE%BA%E5%9D%9B.md)
>
> [**神仙打架看不懂？用人话解读NIPS神经网络攻防赛清华三连冠团队模型算法**](https://github.com/TommyZihao/Zihao-Blog/blob/master/%E7%A5%9E%E4%BB%99%E6%89%93%E6%9E%B6%E7%9C%8B%E4%B8%8D%E6%87%82%EF%BC%9F%E7%94%A8%E4%BA%BA%E8%AF%9D%E8%A7%A3%E8%AF%BBNIPS%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E8%B5%9B%E6%B8%85%E5%8D%8E%E4%B8%89%E8%BF%9E%E5%86%A0%E5%9B%A2%E9%98%9F%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95.md)
>
> [清华大学团队包揽三项冠军，NIPS 2017对抗样本攻防竞赛总结](https://www.leiphone.com/news/201804/WcmoNd6pO4bTQ1yV.html)
>
> [Adversarial Attacks and Defences Competition](https://arxiv.org/abs/1804.00097?utm_campaign=ARCHITECHT&utm_medium=web&utm_source=ARCHITECHT_15)
>
> [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
>
> [Adversarial Examples that Fool both Computer Vision and Time-Limited Humans](https://arxiv.org/abs/1802.08195)
>
> [Goodfellow最新对抗样本，连人类都分不清是狗是猫](https://www.leiphone.com/news/201802/85vvpmuoNRT8gZHD.html)
>
> [Goodfellow最新对抗样本，连人类都分不清是狗是猫](https://www.leiphone.com/news/201802/85vvpmuoNRT8gZHD.html)
>
> [动量迭代攻击和高层引导去噪：对抗样本攻防的新方法](http://www.mooc.ai/open/course/383)
>
> [清华大学廖方舟：产生和防御对抗样本的新方法 | 分享总结](https://www.leiphone.com/news/201801/eqwoT6Q4KFzXtjyy.html)
>
> [两分钟论文：对抗样本同时骗过人类和计算机视觉 @雷锋字幕组](https://v.qq.com/x/page/n0638ta39r3.html?start=88)
>
> [谷歌新论文发现：对抗样本也会骗人](https://www.leiphone.com/news/201804/6ZCljeQmSYKFGetg.html)
>
> 友情链接：
>
> [同济大学开源镜像站](https://mirrors.tongji.edu.cn/)
>
> 西南人工智能爱好者联盟   
> 重庆大学人工智能协会     
>
> 欢迎关注作者的微信公众号：子豪兄的科研小屋。说人话的零基础深度学习、数据科学视频教程、树莓派趣味开发视频教程等你来看！
