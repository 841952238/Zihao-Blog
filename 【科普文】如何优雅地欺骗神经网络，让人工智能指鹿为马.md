> 本文用人话介绍了人工智能与信息安全的交叉前沿研究，包括误导神经网络指鹿为马的对抗样本、针对神经网络的逃逸攻击、NIPS 2017神经网络对抗样本攻防竞赛。部分内容来自于2018中国计算机大会人工智能与信息安全分会场报告。
>
> 作者：张子豪（同济大学在读研究生）   
>
> 微信公众号：子豪兄的科研小屋  回复“指鹿为马”即可看到本文原文
>
> 友情链接：
>
> 同济大学开源软件协会  
> 西南人工智能爱好者联盟   
> 重庆大学人工智能协会     
>
> 发布于2018-10-28

<font color=#0099ff size=6 face="黑体">自己人！本文只是一篇说人话的科普文，没有任何数学推导！</font>



[TOC]



# 1、图片干扰：人工智能秒变人工智障

![用生成对抗神经网络制作表情包：情绪逐渐变化](https://upload-images.jianshu.io/upload_images/13714448-7a888d3a407e4c49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



胖虎和吴亦凡，边界是如此的模糊。

王力宏和张学友，看起来是如此神似。

人脸识别、自动驾驶、刷脸支付、抓捕逃犯……人工智能与实体经济深度结合，彻底改变了我们的生活。神经网络和深度学习貌似强大无比，值得信赖。

殊不知，人工智能是最聪明的，却也是最笨的，其实只要略施小计就能误导最先进的深度学习模型指鹿为马。

## 大熊猫被误判为长臂猿

早在2015年，“生成对抗神经网络GAN之父”Ian Goodfellow在ICLR会议上展示了攻击神经网络欺骗成功的案例，在原版大熊猫图片中加入肉眼难以发现的干扰，生成对抗样本。就可以让Google训练的神经网络误认为它99.3%是长臂猿。

![大熊猫变长臂猿](https://upload-images.jianshu.io/upload_images/13714448-a376255416a12da2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## 阿尔卑斯山被误判为狗

2017NIPS对抗样本攻防竞赛案例：阿尔卑斯山图片篡改后被神经网络误判为狗、河豚被误判为螃蟹。对抗样本不仅仅对图片和神经网络适用，对支持向量机、决策树等算法也同样有效。

![2017NIPS对抗样本攻防竞赛案例：阿尔卑斯山图片篡改后被神经网络误判为狗、河豚被误判为螃蟹。对抗样本不仅仅对图片和神经网络适用，对SVM、决策树等算法也有效](https://upload-images.jianshu.io/upload_images/13714448-073423e4463b0068.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 人工智障：人工也是智障

在2018年，Ian Goodfellow再发大招，不仅欺骗了神经网络，还能欺骗人眼。详情见论文[Adversarial Examples that Fool both Computer Vision and Time-Limited Humans](https://arxiv.org/abs/1802.08195)，文中提出了首个可以欺骗人类的对抗样本。下图左图为猫咪原图，经过干扰之后生成右图，神经网络和人眼都认为是狗。

> 这篇论文行文流畅、通俗易懂，适合新手阅读。你也可以用两分钟观看这篇论文的[视频介绍](https://v.qq.com/txp/iframe/player.html?vid=n0638ta39r3)。  

![左图为原图猫咪，经过干扰之后生成右图，神经网络和人眼都认为是狗](https://static.leiphone.com/uploads/new/article/740_740/201802/5a927ff3eddaa.png?imageMogr2/format/jpg/quality/90)

​    下图中，绿色框为猫的原图。左上角显示了攻击的目标深度模型数量越多，生成的图像对人类来说越像狗。左下角显示了针对 10 个模型进行攻击而生成的对抗样本，当eps=8的时候，人类受试者已经把它认成狗了。

![猫还是狗？](https://static.leiphone.com/uploads/new/article/740_740/201802/5a929f8d339d4.png?imageMogr2/format/jpg/quality/90)





# 2、对抗样本：让神经网络指鹿为马

这就是对机器学习模型的逃逸攻击，它能绕过深度学习的判别并生成欺骗结果。**攻击者在原图上构造的修改被称为“对抗样本”**。神经网络对抗样本生成与攻防是一个非常有（zhuang）趣（bi）且有前景的研究方向，但常人难以轻易理解内在原理，下面子豪兄就用人话向你娓娓道来。
> 这是一篇介绍对抗样本生成基本原理的通俗易懂的文章：[对抗样本的基本原理](https://www.leiphone.com/news/201806/aLeiPZA0FbVtQI6M.html)，里面甚至教你用开源人工智能框架Keras生成对抗样本攻击知名的Inception V3模型，把猪识别成烤面包机。

![逃逸攻击](http://blogs.360.cn/wp-content/uploads/2017/10/1-2-1024x450.png)

除此之外，人工智能还面临模型推断攻击、拒绝服务攻击、传感器攻击等多种信息安全挑战。

## 白盒攻击与黑盒攻击

逃逸攻击可分为白盒攻击和黑盒攻击。白盒攻击是在已经获取机器学习模型内部的所有信息和参数上进行攻击，令损失函数最大，直接计算得到对抗样本；黑盒攻击则是在神经网络结构为黑箱时，仅通过模型的输入和输出，逆推生成对抗样本。下图左图为白盒攻击（自攻自受），右图为黑盒攻击（用他山之石攻此山之玉）。

  ![白盒攻击与黑盒攻击](https://upload-images.jianshu.io/upload_images/13714448-2403524823c839b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​    对抗样本不仅仅对模型的最后预测产生误导，而是从特征提取过程就产生误导，下图展示了第147号神经元在正常深度学习模型和对抗样本中的关注区域。在正常模型中，第147号神经元重点关注小鸟的头部信息。在对抗样本中，第147号神经元则完全被误导了，关注的区域杂乱无章。同时也说明，**对抗样本不是根据语义生成的**，它并不智能。



## 对抗样本改变了图像的什么？噪音！

  对抗样本会在原图上增加肉眼很难发现的干扰，但依旧能看得出来和原图的区别，下图左图为对抗样本，右图为熊猫
![干扰样本对图像的改变](https://upload-images.jianshu.io/upload_images/13714448-c0a686a55b828825.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

对抗样本不仅仅对模型的最后预测产生误导，而是从特征提取过程就产生误导，下图展示了第147号神经元在正常深度学习模型和对抗样本中的关注区域。在正常模型中，第147号神经元重点关注小鸟的头部信息。在对抗样本中，第147号神经元则完全被误导了，关注的区域杂乱无章。同时也说明，**对抗样本只是由简单粗暴的计算生成，并不是根据真实语义生成的**，它并不智能。而且，正如接下来讲述的，对抗样本对图片预处理过程非常敏感，任何区域截图、放大缩小、更换模型都很容易让对抗样本失效。

 ![对抗样本不仅仅对最后的预测产生误导，而是从特征提取开始就产生误导](https://upload-images.jianshu.io/upload_images/13714448-73a71d45ff8a78fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## 对抗样本有多强？截个图换个模型就失效！

​    其实，如果你把那张经过攻击篡改之后的大熊猫图片稍微放大或缩小，或者直接截一部分图，然后放到其它公开的图像识别模型上运行（比如百度识图），识别结果依旧是大熊猫。这意味着对抗样本仅对指定的图片和攻击模型生效，对诸如区域截图、放大缩小之类的预处理过程是非常敏感的。也就是说，如果还想欺骗更多其它的深度学习模型，就要通过逆推破解不同深度学习模型的图片预处理方法，找到发生图像识别错误的那种预处理方法，然后应用在原图片上。
![用百度识图识别经过对抗样本攻击之后的大熊猫图片，识别结果依旧是大熊猫](https://upload-images.jianshu.io/upload_images/13714448-14e6c241fb00614e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



# 3、NIPS 2017 神经网络对抗攻防竞赛

2017年，“生成对抗神经网络GAN之父”Ian Goodfellow 牵头组织了NIPS的 Adversarial Attacks and Defences（神经网络对抗攻防竞赛），清华大学博士生董胤蓬、廖方舟、庞天宇及指导老师朱军、胡晓林、李建民、苏航组成的团队在竞赛中的全部三个项目中得到冠军。以下是清华大学参赛师生赛后撰写的总结和相关报告。

> [清华大学团队包揽三项冠军，NIPS 2017对抗样本攻防竞赛总结](https://www.leiphone.com/news/201804/WcmoNd6pO4bTQ1yV.html)    
>
> [清华大学廖方舟：产生和防御对抗样本的新方法 | 分享总结](https://www.leiphone.com/news/201801/eqwoT6Q4KFzXtjyy.html)    
>
> [朱军：深度学习中的对抗攻击与防守—2018中国计算机大会人工智能与信息安全分会场](https://github.com/TommyZihao/Zihao-Blog/blob/master/2018%E4%B8%AD%E5%9B%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E4%BC%9A%EF%BC%9A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E5%88%86%E8%AE%BA%E5%9D%9B.md)    
>
> [动量迭代攻击和高层引导去噪：对抗样本攻防的新方法](http://www.mooc.ai/open/course/383)     
>
> [清华参赛队攻击组论文：Boosting Adversarial Attacks with Momentum](https://arxiv.org/abs/1710.06081)   
>
> [清华参赛队防御组论文：Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser](https://arxiv.org/abs/1712.02976)   

神仙打架看不懂？请看本文作者张子豪撰写的另一篇人类能看得懂的科普文：[神仙打架看不懂？用人话解读NIPS神经网络攻防赛清华三连冠团队模型算法](https://github.com/TommyZihao/Zihao-Blog/blob/master/%E7%A5%9E%E4%BB%99%E6%89%93%E6%9E%B6%E7%9C%8B%E4%B8%8D%E6%87%82%EF%BC%9F%E7%94%A8%E4%BA%BA%E8%AF%9D%E8%A7%A3%E8%AF%BBNIPS%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E8%B5%9B%E6%B8%85%E5%8D%8E%E4%B8%89%E8%BF%9E%E5%86%A0%E5%9B%A2%E9%98%9F%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95.md)。这篇文章详细介绍了基本算法FGSM、比赛规则、清华参赛队的模型可迁移性优化策略、降噪优化算法。



# 4、邪恶的未来应用

## 攻击汽车音乐，让驾驶系统得到错误语音指令（已实现）

2018年10月25日，杭州，中国计算机大会，人工智能与信息安全分会。

第三位报告人是中国科学院大学陈恺教授。分享题目是“人工智能时代下的安全攻防”。本次报告讲述了人工智能在网络安全工作中的应用，并通过对歌曲进行干扰生成错误微信语音指令进而发动攻击的例子，讲述了人工智能算法的脆弱性与未来攻防发展趋势。

> 陈恺教授简介：中国科学院信息工程研究所研究员，中国科学院大学教授、博士生导师。信息安全国家重点实验室副主任，《Cybersecurity》编辑部主任。国家“万人计划”青年拔尖人才、北京市“科技新星”。2010年获中国科学院研究生博士学位，美国宾州州立大学博士后。中国保密协会隐私保护专业委员会委员，中国计算机学会系统软件专委会委员。主要研究领域包括软件与系统安全、人工智能安全。在IEEE S&P、USENIX Security、ACM CSS、ICSE、ASE等发表论文70余篇；曾主持和参加国家重点研发计划、国家自然科学基金、863计划等国家部委课题40余项。

在报告中，陈恺教授展示了他们的最新成果：对汽车音响播放的歌曲进行干扰编码，在人耳听起来仍然是原曲的情况下就可以让微信的语音输入法获得错误的“Open the door”指令。但歌曲很容易受外界噪音干扰。本文作者张子豪提出可以使用树莓派微型电脑发射FM调频广播播放干扰之后的歌曲，直接干扰汽车收音机，陈恺博士高度赞赏了这个建议并表示他们已经尝试过这个方法，但决定干扰成功率的关键还是在于过滤外界噪音干扰。

## 攻击人脸识别，让刷脸支付和门禁系统危机重重

## 攻击交通标志，对自动驾驶汽车识别出错误含义



# 5、参考文献与扩展阅读

> [**2018中国计算机大会：人工智能与信息安全分论坛**](https://github.com/TommyZihao/Zihao-Blog/blob/master/2018%E4%B8%AD%E5%9B%BD%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%A4%A7%E4%BC%9A%EF%BC%9A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E5%88%86%E8%AE%BA%E5%9D%9B.md)
>
> [**神仙打架看不懂？用人话解读NIPS神经网络攻防赛清华三连冠团队模型算法**](https://github.com/TommyZihao/Zihao-Blog/blob/master/%E7%A5%9E%E4%BB%99%E6%89%93%E6%9E%B6%E7%9C%8B%E4%B8%8D%E6%87%82%EF%BC%9F%E7%94%A8%E4%BA%BA%E8%AF%9D%E8%A7%A3%E8%AF%BBNIPS%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E8%B5%9B%E6%B8%85%E5%8D%8E%E4%B8%89%E8%BF%9E%E5%86%A0%E5%9B%A2%E9%98%9F%E6%A8%A1%E5%9E%8B%E7%AE%97%E6%B3%95.md)
>
> [清华大学团队包揽三项冠军，NIPS 2017对抗样本攻防竞赛总结](https://www.leiphone.com/news/201804/WcmoNd6pO4bTQ1yV.html)
>
> [Adversarial Attacks and Defences Competition](https://arxiv.org/abs/1804.00097?utm_campaign=ARCHITECHT&utm_medium=web&utm_source=ARCHITECHT_15)
>
> [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
>
> [Adversarial Examples that Fool both Computer Vision and Time-Limited Humans](https://arxiv.org/abs/1802.08195)
>
> [Goodfellow最新对抗样本，连人类都分不清是狗是猫](https://www.leiphone.com/news/201802/85vvpmuoNRT8gZHD.html)
>
> [Goodfellow最新对抗样本，连人类都分不清是狗是猫](https://www.leiphone.com/news/201802/85vvpmuoNRT8gZHD.html)
>
> [动量迭代攻击和高层引导去噪：对抗样本攻防的新方法](http://www.mooc.ai/open/course/383)
>
> [清华大学廖方舟：产生和防御对抗样本的新方法 | 分享总结](https://www.leiphone.com/news/201801/eqwoT6Q4KFzXtjyy.html)
>
> [两分钟论文：对抗样本同时骗过人类和计算机视觉 @雷锋字幕组](https://v.qq.com/x/page/n0638ta39r3.html?start=88)
>
> [谷歌新论文发现：对抗样本也会骗人](https://www.leiphone.com/news/201804/6ZCljeQmSYKFGetg.html)
>
> 欢迎关注作者的微信公众号：子豪兄的科研小屋。说人话的的深度学习、数据挖掘视频教程、树莓派趣味开发视频教程等你来看！
>
