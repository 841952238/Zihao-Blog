>本文为2018年10月25日杭州中国计算机大会：人工智能与信息安全分会场两位顶尖专家报告内容汇总。涉及人工智能应用中的神经网络攻击、预测结果欺骗、模型逆向破解，及移动设备AI芯片未来的信息安全挑战。       
>作者：张子豪   
>同济大学开源软件协会  
>西南人工智能爱好者联盟   
>重庆大学人工智能协会     



![2018中国计算机大会：人工智能与信息安全分会场](https://upload-images.jianshu.io/upload_images/13714448-6fb364b530b76cb9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

[TOC]

## 1、分享嘉宾介绍：

### 朱军

清华大学计算机系教授，清华大学人工智能研究院院长助理。清华计算机本科、博士，卡耐基梅隆大学博士后。主要从事机器学习基础理论算法及应用研究。入选国家“万人计划”青年拔尖人才。

2017，Ian Goodfellow 等研究者牵头组织了NIPS的 Adversarial Attacks and Defences（对抗攻击防御）竞赛，清华大学博士生董胤蓬、廖方舟、庞天宇及指导老师朱军、胡晓林、李建民、苏航组成的团队在竞赛中的全部三个项目中得到冠军，赛后总结请看[清华大学团队包揽三项冠军，NIPS 2017对抗样本攻防竞赛总结](https://www.leiphone.com/news/201804/WcmoNd6pO4bTQ1yV.html)。

### 李康

360智能安全研究院负责人。清华计算机本科，耶鲁大学法学硕士，俄勒冈研究院计算机博士。主要研究系统与网络安全。李博士是网络安全对抗赛CTF最早的实践者，他是XCTF联赛的联合发起人，并担任清华大学蓝莲花战队的启蒙老师。

![2017NIPS对抗样本攻防竞赛案例：阿尔卑斯山图片篡改后被神经网络误判为狗、河豚被误判为螃蟹。对抗样本不仅仅对图片和神经网络适用，对SVM、决策树等算法也有效](https://upload-images.jianshu.io/upload_images/13714448-073423e4463b0068.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



## 2、图片干扰：人工智能秒变人工智障

​    一张赵本山的照片，神经网络却咬定图片中是李冰冰？    
​    一张小绵羊的图片，深度学习却推断出九成是大灰狼？    
​    你带着老婆，出了城，坐着无人驾驶汽车，吃着火锅还唱着歌，突然车门大开，汽车开始狂飙!     
![吃着火锅唱着歌](https://upload-images.jianshu.io/upload_images/13714448-1c0857afd52ea95e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640)

​    而远在千里外的吃瓜群众却收到了一张超速罚单。     

​    这不是科幻片中的情节，而是眼前的现实。这一切的原因很简单：图片经过不经意的微小修改就可让神经网络的预测面目全非。汽车把你唱的歌识别成开门并加速的指令，路旁测速的车牌识别系统把你的车牌认成了吃瓜群众的车牌。    
​    你曾经认为值得以命相托的深度学习和神经网络模型很容易遭受人为攻击，导致错误的识别结果！人工智能也能秒变人工智障。     
​    人工智能与信息安全领域的顶尖学者相聚在2018中国计算机大会——人工智能与信息安全分论坛，共话信息安全的“芯”挑战。     

## 3、对抗样本：让神经网络指鹿为马

​    以下内容整理自朱军教授和李康博士的报告分享：     

### 逃逸攻击与对抗样本

​    早在2015年，“生成对抗神经网络GAN之父”Ian Goodfellow在ICLR会议上展示了攻击神经网络欺骗成功的案例，在原版大熊猫图片中加入肉眼难以发现的干扰，生成对抗样本。就可以让Google训练的神经网络误认为它99.3%是长臂猿。

![大熊猫变长臂猿](https://upload-images.jianshu.io/upload_images/13714448-a376255416a12da2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​    

​    这就是对机器学习系统的逃逸攻击，它能绕过深度学习的判别并生成欺骗结果。攻击者在原图上构造的修改被称为“对抗样本”。逃逸攻击可分为白盒攻击和黑盒攻击。白盒攻击是在已经获取机器学习模型内部的所有信息和参数上进行攻击，令损失函数最大，直接计算得到对抗样本；黑盒攻击则是在神经网络结构为黑箱时，仅通过模型的输入和输出，逆推生成对抗样本。

![白盒攻击和黑盒攻击](D:\2018秋季学期\百度自媒体\1540574840008.png)

> 这是一篇介绍对抗样本生成基本原理的通俗易懂的文章：[对抗样本的基本原理](https://www.leiphone.com/news/201806/aLeiPZA0FbVtQI6M.html)，里面甚至教你如何用Keras生成对抗样本攻击Inception V3神经网络，把家猪识别成烤面包机。

​    除此之外，人工智能系统还会面对模型推断攻击、拒绝服务攻击、传感器攻击等多种信息安全挑战。
![逃逸攻击](http://blogs.360.cn/wp-content/uploads/2017/10/1-2-1024x450.png)

​    在2018年，Ian Goodfellow再发大招，不仅欺骗了神经网络，还能欺骗人眼。详情见论文[Adversarial Examples that Fool both Computer Vision and Time-Limited Humans](https://arxiv.org/abs/1802.08195)，文中提出了首个可以欺骗人类的对抗样本。下图左图为猫咪原图，经过干扰之后生成右图，神经网络和人眼都认为是狗。

>这篇论文行文流畅、通俗易懂，适合新手阅读。你也可以用两分钟观看这篇论文的[视频介绍](https://v.qq.com/txp/iframe/player.html?vid=n0638ta39r3)。  

![左图为原图猫咪，经过干扰之后生成右图，神经网络和人眼都认为是狗](https://static.leiphone.com/uploads/new/article/740_740/201802/5a927ff3eddaa.png?imageMogr2/format/jpg/quality/90)

​    下图中，绿色框为猫的原图。左上角显示了攻击的目标深度模型数量越多，生成的图像对人类来说越像狗。左下角显示了针对 10 个模型进行攻击而生成的对抗样本，当eps=8的时候，人类受试者已经把它认成狗了。

![猫还是狗？](https://static.leiphone.com/uploads/new/article/740_740/201802/5a929f8d339d4.png?imageMogr2/format/jpg/quality/90)

​    对抗样本不仅仅对模型的最后预测产生误导，而是从特征提取过程就产生误导，下图展示了第147号神经元在正常深度学习模型和对抗样本中的关注区域。在正常模型中，第147号神经元重点关注小鸟的头部信息。在对抗样本中，第147号神经元则完全被误导了，关注的区域杂乱无章。同时也说明，**对抗样本不是根据语义生成的**，它并不智能。

 ![不仅仅对最后的预测产生误导，而是从特征提取开始就产生误导](https://upload-images.jianshu.io/upload_images/13714448-73a71d45ff8a78fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 降噪优化策略

​    对抗样本会在原图上增加肉眼很难发现的干扰，但依旧能看得出来和原图的区别：
![干扰样本对图像的改变](https://upload-images.jianshu.io/upload_images/13714448-c0a686a55b828825.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
​    一个很自然的想法就是，通过像素级别的去噪，把干扰样本图片还原回原来的图片，但经过试验，发现成功率很低。
![传统去噪成功率很低](https://upload-images.jianshu.io/upload_images/13714448-fcb3230bb04f5984.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
​    研究人员进而把去噪的目标从像素级别还原回原来的图片，改为模型能被正确识别，成功率大大提高，见下图黑色LGD线。
![改进去噪方式之后，成功率大大提高](https://upload-images.jianshu.io/upload_images/13714448-5015a0605adece99.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
​    

### 对抗样本对预处理方式的敏感性

​    其实，如果你把那张经过攻击篡改之后的大熊猫图片稍微放大或缩小，或者直接截一部分图，然后放到其它公开的图像识别模型上运行（比如百度识图），识别结果依旧是大熊猫。这意味着对抗样本仅对指定的图片和攻击模型生效，对诸如区域截图、放大缩小之类的预处理过程是非常敏感的。也就是说，如果还想欺骗更多其它的深度学习模型，就要通过逆推破解不同深度学习模型的图片预处理方法，找到发生图像识别错误的那种预处理方法，然后应用在原图片上。
![用百度识图识别篡改之后的大熊猫图片，识别结果依旧是大熊猫](https://upload-images.jianshu.io/upload_images/13714448-14e6c241fb00614e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​    

### 黑盒攻击算法优化

​    黑盒攻击非常依赖对抗样本的可迁移性，而且存在过拟合的情况，如下图所示，白盒攻击的成功率随迭代次数增加很快达到接近100%，黑盒攻击则存在明显的过拟合。
![白盒攻击和黑盒攻击的可收敛性](https://upload-images.jianshu.io/upload_images/13714448-d620a0dede80a9eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
​    使用最简单的FGSM优化算法，引入Momentum，加快收敛速度。FGSM算法详细实现过程请看[清华大学团队包揽三项冠军，NIPS 2017对抗样本攻防竞赛总结](https://www.leiphone.com/news/201804/WcmoNd6pO4bTQ1yV.html)。神仙打架看不懂？请看子豪兄总结的人类能听懂版：

​    下图展示了FGSM算法的基本原理，X*是要产生的对抗样本，x是真实样本，y是正确的预测值。

​    第一行表示构造损失函数L(x\*,y)，同时保证新生成的对抗样本x\*必须与原图x保持在一定距离的高维空间之内（约束条件）。 在数学上，argmax(f(x))是使得 f(x)取得最大值所对应的变量x。第一行就是说在满足约束条件前提下，找到让损失函数L(x\*,y)最大（也就是让神经网络推测结果越失败）的对抗样本x\*。

> **用人话说就是：在肉眼看上去依旧是同一只大熊猫图片的基础上，把神经网络的推测结果能骗多远骗多远。**

![FGSM优化算法：第一行表示构造损失函数，使得x*必须与x保持在一定距离的高维空间之内](https://upload-images.jianshu.io/upload_images/13714448-aadaab7443cd5a1f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​    第二行：运用线性假设，构造x\*的迭代过程，相当于用反向传播的思想不断用新生成 把反传给图像上的梯度传给原图像

​    第三行：之所以不采用L2 norm，是因为会产生很大的畸变。

​    只后，采用多步FGSM攻击，使损失函数最大化，每一步迭代的步长会相应减小。

![多步FGSM攻击](https://upload-images.jianshu.io/upload_images/13714448-7ec1021378dfbfa9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



​    通过上述步骤，你已经可以让神经网络不认识熊猫了，那如果我想让神经网络把熊猫认成长臂猿呢？就要用到Targeted FGSM攻击，只需用预测输出结果长臂猿y\*取代传统FGSM算法中的正确预测结果y，同时最小化损失函数即可。

![目标FGSM攻击](https://upload-images.jianshu.io/upload_images/13714448-4ad3199938b96bd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![FGSM优化算法](https://upload-images.jianshu.io/upload_images/13714448-40c751d07e4e67fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​    下图演示了FGSM优化算法结果，红框内为白盒攻击成功率：
![FGSM优化算法结果](https://upload-images.jianshu.io/upload_images/13714448-409e195ad5d9feb8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
​    攻击Inception V3模型
![攻击Inception V3模型](https://upload-images.jianshu.io/upload_images/13714448-b046c3c2d2a26078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
​    经过优化，样本空间中的各种元素距离变大了，意味着指鹿为马的难度也加大了。
![各样本之间距离变大](https://upload-images.jianshu.io/upload_images/13714448-5a4ebe06fe05c2b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​    对抗样本并不是随机的噪音，而是在原图的边缘和轮廓出生成。
![对抗样本在图像边缘轮廓处生成](https://upload-images.jianshu.io/upload_images/13714448-573fcef7cb4d2693.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

​    那么，这种对抗样本是智能生成的吗？是根据图片语义信息生成的吗？答案：不是的，这种攻击非常简单粗暴，并不是根据图片的语义信息进行智能伪装欺骗，同时根据上文也可以看到，逃逸攻击对图像预处理过程非常敏感。




## 4、移动智能终端深度学习模型的安全

​    人工智能与信息安全的第二个热点：深度学习模型参数被窃取的风险和数据安全。
​    随着边缘计算和智能移动终端时代的到来，在移动终端部署本地AI应用越来越广泛，从iPhone X的刷脸解锁，到华为、高通部署手机端的AI芯片。在移动终端本地运行AI应用，可有效解决延迟、传输带宽、用户隐私泄露等问题，但同时也带来本地深度学习模型的数据安全问题。经过简单的逆推，就可以破解很多本地的AI应用，甚至可以知道其中的Caffe模型的基本参数，有些开发者会采用AES加密把模型封装起来，但殊不知在AES密钥也得保存在本地文件中。有时甚至根据追踪AI应用对内存的访问情况，就可以判断出这个模型的神经网络结构。所以AI开发者在向移动端和嵌入式设备中部署AI应用时，一定要事先请教安全团队，确保模型数据安全。

![人工智能模型数据安全](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1540498892027&di=e256bca025813be17acc3c1819527b34&imgtype=0&src=http%3A%2F%2F03imgmini.eastday.com%2Fmobile%2F20180916%2F20180916180412_19afbee60d624332f0db6fb897b9e01f_1.jpeg)



## 5、现场观众提问

### 小信息量数据的对抗样本生成

- 问：讲座中提到的神经网络攻击，都是针对图片、语音这些包含丰富冗余信息量数据的。那么对于小信息量的数据，比如二维码图片，能否生成对抗样本呢？
- 答：这个问题提的很好。图片、语音数据恰恰是因为包含丰富的冗余信息，在加入对抗样本之后才不会被人察觉，而二维码这种数据稍加篡改就会被肉眼察觉。而且在安全领域，已经有许多成熟的针对二维码的攻击方案，最原始的就是有人把摩拜单车上的二维码撕下来换成自己的收款码，或者把别人的付款码打印下来用于消费。

### 污染训练模型用的数据集进行攻击

- 问：能否直接引入大量污染的数据，针对深度学习训练用的数据集进行攻击？
- 答：可以。这是釜底抽薪的攻击，也叫“后门攻击”。Machine Learning和Machine Teaching从来不是割裂的。在防御角度，需要“纵深防御”，同时这也是“连续学习”的一部分。



## 6、参考文献与扩展阅读

> [清华大学团队包揽三项冠军，NIPS 2017对抗样本攻防竞赛总结](https://www.leiphone.com/news/201804/WcmoNd6pO4bTQ1yV.html)
>
> [Adversarial Attacks and Defences Competition](https://arxiv.org/abs/1804.00097?utm_campaign=ARCHITECHT&utm_medium=web&utm_source=ARCHITECHT_15)
>
> [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
>
> [Adversarial Examples that Fool both Computer Vision and Time-Limited Humans](https://arxiv.org/abs/1802.08195)
>
> [Goodfellow最新对抗样本，连人类都分不清是狗是猫](https://www.leiphone.com/news/201802/85vvpmuoNRT8gZHD.html)
>
> [Goodfellow最新对抗样本，连人类都分不清是狗是猫](https://www.leiphone.com/news/201802/85vvpmuoNRT8gZHD.html)
>
> [动量迭代攻击和高层引导去噪：对抗样本攻防的新方法](http://www.mooc.ai/open/course/383)
>
> [清华大学廖方舟：产生和防御对抗样本的新方法 | 分享总结](https://www.leiphone.com/news/201801/eqwoT6Q4KFzXtjyy.html)
>
> [两分钟论文：对抗样本同时骗过人类和计算机视觉 @雷锋字幕组](https://v.qq.com/x/page/n0638ta39r3.html?start=88)
>
> [谷歌新论文发现：对抗样本也会骗人](https://www.leiphone.com/news/201804/6ZCljeQmSYKFGetg.html)
>
> []()
>
> []()
>
> []()
>
> []()
>
>


> 欢迎关注作者的微信公众号：子豪兄的科研小屋。原创的深度学习视频教程、树莓派趣味开发视频教程等你来看！
